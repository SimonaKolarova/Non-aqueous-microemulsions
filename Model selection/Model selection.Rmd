---
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, cache = TRUE, warning=FALSE, message=FALSE)
```

```{r libraries, echo = FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(kernlab)
library(nnet)
library(pROC)
library(doParallel)
```

### Scripts
```{r r scripts}
source("Machine learning algorithms.R", local = knitr::knit_global())
source("Predictive performance assessment.R", local = knitr::knit_global())
source("Statistical analysis.R", local = knitr::knit_global())
```

```{r parallel processing, echo = FALSE}
registerDoParallel(makeCluster(detectCores()-1))
```

### Dataset
``` {r data}
PB_predictors_data<- read.csv("Data/Phase behaviour and descriptors (25 pc step).csv")
```

The dataset was split into a training/testing (`TT_data`) and validation (`V_data`) subset comprising 21 and 7 phase diagrams, respectively.
``` {r data splitting}
TT_data <- filter(PB_predictors_data, Set == "Training")
V_data <- filter(PB_predictors_data, Set == "Validation")
```

A list of all phase behaviour predictor names was created and the phase behaviour variable was encoded as a factor.
``` {r data definitions}
predictors_all<-colnames(TT_data[,3:66][,-(4:6)])

TT_data$Phase.behaviour<-factor(as.character(TT_data$Phase.behaviour))
V_data$Phase.behaviour<-factor(as.character(V_data$Phase.behaviour))
```

### Predictive modelling 
#### Model training and evaluation
**Global parameters** <br>
A *summary* and a *training control* function were defined to allow the uniform training and evaluation of the models.
```{r controls}
PMsummary <- function(...) c(twoClassSummary(...), defaultSummary(...))
trainCtrl <- trainControl(method = "cv", 
                          number = 10,
                          summaryFunction = PMsummary, 
                          classProbs = TRUE, 
                          savePredictions=TRUE)
```

```{r results dataframe, echo = FALSE}
model_performance <- data.frame(matrix(ncol = 17, nrow = 80))
colnames(model_performance) <- c("Model", "AUC_testing", "Accuracy_testing", "Kappa_testing", "Sensitivity_testing", "Specificity_testing", "Precision_testing", "Recall_testing", "F1_testing", "AUC_validation", "Accuracy_validation", "Kappa_validation", "Sensitivity_validation", "Specificity_validation", "Precision_validation", "Recall_validation", "F1_validation")
```

**Logistic regression (LR)** <br>
LR is a linear classification algorithm, which models the
mathematical relationships between each predictor and the categorical outcome by
minimising the sums of the squared residuals and linearly aggregates the resulting individual
relationships to obtain a probabilistic categorical outcome. This ML algorithm requires no
parameter tuning, thereby reducing the likelihood of data overfitting. For the purpose of
this study, the *glm* method in the R package *caret* was used to train a LR model.

```{r LR}
for (i in 1:20) {
  model_performance <- run_logistic_regression(TT_data, V_data, predictors_all, model_performance,"Logistic regression", i)
}
```
**Random forest (RF)** <br>
RF is a tree-based ensemble model that constructs individual trees by
using the independent variables to create binary decision split points, *i.e.*, nodes, and
partitions the data into higher outcome homogeneity groups until a user-defined maximum
number of terminal tree nodes or minimum outcome group size is reached. To reduce the
correlation between the structure of the individual trees in the RF ensemble, each decision
tree is trained using a training data subset (bootstrap sampling) and a predictors subset
(feature bagging). Once trained, the model makes outcome predictions by averaging the
predictions of the independent trees. In this study, we used the R package `randomForest`
to train a RF ensemble with 1000 trees, each of which was trained using approximately 63%
of the training data and between 10 and 61 predictors (optimum number determined by
tuning) to obtain a final tree structure with a maximum of 15 tree nodes and a minimum of 1
datapoint per terminal node.

```{r RF}
for (i in 21:40) {
  model_performance <- run_random_forest(TT_data, V_data, predictors_all, model_performance, "Random forest", i)
}
```

**Support vector machines (SVM)** <br>
SVM is a non-probabilistic classifier, which generates a multidimensional space using the predictors, defines the coordinates of the outcome data in it and
determines the optimum separation hyperplane between the different outcome classes. To
optimise the separation between the dependent variable categories, while minimising data
overfitting, both the kernel type (*i.e.*, the spatial transformation function) and the cost (*i.e.*,
the penalty for misclassified samples) can be altered. To obtain a SVM predictive model,
we used the R package `kernlab`, specified the kernel function as radial and tuned the value
of the cost parameter.
```{r SVM}
for (i in 41:60) {
  model_performance <- run_support_vector_machines(TT_data, V_data, predictors_all, model_performance, "Support vector machines", i)
}
```

**Neural network (NN)** <br>
NN is a machine learning algorithm comprised of simple computing units, *i.e.*, neurons,
organised in interconnected layers, *i.e.*, network, where the predictor layer input passes
through one or more hidden layers before reaching the output layer and determining the
dependent variable value. The algorithmâ€™s architecture, the number of hidden neurons and
layers and the weight decay, *i.e.*, the penalisation for large regression coefficients, among
others, can all be adjusted to optimise the NN predictive performance. In this study, we
used the R package `nnet` to train a NN model with a feed-forward architecture and a
single hidden layer. The optimum number of hidden neurons and the weight decay value
were determined through parameter tuning.

```{r NN}
for (i in 61:80) {
  model_performance <- run_neural_network(TT_data, V_data, predictors_all, model_performance, "Neural network", i)
}
```

#### Model performance comparison 
To select the best model for the classification problem, the area under the receiver operating characteristic (ROC) curve (AUC) for the predictions on the testing (randomly selected 30% of `TT_data`, which was not used for training) and on the validation subsets for the four different models was compared.
```{r performance metrics, echo = FALSE}
model_performance$Model <- factor(model_performance$Model)
model_performance$AUC_testing <- as.numeric(model_performance$AUC_testing)
model_performance$AUC_validation <- as.numeric(model_performance$AUC_validation)
```

**Visual comparison**
```{r models plot, fig.height=4, fig.width=10, echo=FALSE}
ggplot(model_performance, aes(x = Model)) + 
  geom_boxplot(aes(y=AUC_testing, colour = "Testing subset")) + 
  geom_boxplot(aes(y=AUC_validation, colour = "Validation subset")) +
  scale_x_discrete(limits=c("Logistic regression", "Random forest", "Support vector machines", "Neural network"))  +
  ylim(0.5,1) +
  labs(title = "Predictive performance of different models",
       y = "Area under the ROC curve (AUC)",
       color = "Metric") +
  scale_color_brewer(palette = "Set1") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) 
```


**Statistical comparison** <br>
Comparison of AUC testing values for the different models:
```{r models testing statistics, echo = FALSE}
run_statistical_analysys(model_performance, model_performance$AUC_testing, model_performance$Model)
```
<br>
Comparison of AUC validation values for the different models:
```{r models validation statistics, echo = FALSE}
run_statistical_analysys(model_performance, model_performance$AUC_validation, model_performance$Model)
```

**Overall, the random forest model was observed to have a statistically significantly better predictive performance on both the testing and validatation datasets than the Logistic regression, Support vector machines or Neural network models and, hence, was carried forward.**


```{r export, echo = FALSE}
write.csv(model_performance, "Results/Model performance.csv", row.names = FALSE)
```