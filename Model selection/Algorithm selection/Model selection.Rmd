---
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, cache = TRUE, warning=FALSE, message=FALSE)
```

```{r libraries, echo = FALSE}
library(tidyverse)
library(reshape)
library(caret)
library(randomForest)
library(kernlab)
library(nnet)
library(pROC)
library(doParallel)
```

### Scripts
```{r r scripts}
source("Machine learning algorithms.R", local = knitr::knit_global())
source("Predictive performance assessment.R", local = knitr::knit_global())
source("Statistical analysis.R", local = knitr::knit_global())
```

```{r parallel processing, echo = FALSE}
registerDoParallel(makeCluster(detectCores()-1))
```

### Dataset
```{r data}
PB_predictors_data<- read.csv("Data/Phase behaviour and descriptors (25 pc step).csv")
```

The dataset was split into a training/testing (`TT_data`) and validation subset comprising 21 and 7 phase diagrams, respectively. Only the training/testing dataset was used for model selection.
```{r data splitting}
TT_data <- filter(PB_predictors_data, Set == "Training")
```

A list of all phase behaviour predictor names was created and the phase behaviour variable was encoded as a factor.
```{r data definitions}
predictors_all<-colnames(TT_data[,3:66][,-(4:6)])
TT_data$Phase.behaviour<-factor(as.character(TT_data$Phase.behaviour))
```

### Predictive modelling 
#### Model training and evaluation
**Global parameters** <br>
A *summary* and a *training control* function were defined to allow the uniform training and evaluation of the models.
```{r controls}
PMsummary <- function(...) c(twoClassSummary(...), defaultSummary(...))
trainCtrl <- trainControl(method = "cv", 
                          number = 10,
                          summaryFunction = PMsummary, 
                          classProbs = TRUE, 
                          savePredictions=TRUE)
```

```{r results dataframe, echo = FALSE}
model_performance <- data.frame(matrix(ncol = 8, nrow = 80))
colnames(model_performance) <- c("Model", "AUC", "Accuracy", "Kappa", "Sensitivity", "Specificity", "Precision", "Fscore")
```

**Logistic regression (LR)** <br>
LR is a linear classification algorithm, which models the
mathematical relationships between each predictor and the categorical outcome by
minimising the sums of the squared residuals and linearly aggregates the resulting individual
relationships to obtain a probabilistic categorical outcome. This ML algorithm requires no
parameter tuning, thereby reducing the likelihood of data overfitting. For the purpose of
this study, the `glm` method in the R package *caret* was used to train a LR model.

```{r LR}
for (i in 1:20) {
  model_performance <- run_logistic_regression(TT_data, predictors_all, model_performance, "LR", i)
}
```
**Random forest (RF)** <br>
RF is a tree-based ensemble model that constructs individual trees by
using the independent variables to create binary decision split points, *i.e.*, nodes, and
partitions the data into higher outcome homogeneity groups until a user-defined maximum
number of terminal tree nodes or minimum outcome group size is reached. To reduce the
correlation between the structure of the individual trees in the RF ensemble, each decision
tree is trained using a training data subset (bootstrap sampling) and a predictors subset
(feature bagging). Once trained, the model makes outcome predictions by averaging the
predictions of the independent trees. In this study, we used the R package `randomForest`
to train a RF ensemble with 1000 trees, each of which was trained using approximately 63%
of the training data and between 10 and 61 predictors (optimum number determined by
tuning) to obtain a final tree structure with a maximum of 15 tree nodes and a minimum of 1
datapoint per terminal node.

```{r RF}
for (i in 21:40) {
  model_performance <- run_random_forest(TT_data, predictors_all, model_performance, "RF", i)
}
```

**Support vector machines (SVM)** <br>
SVM is a non-probabilistic classifier, which generates a multidimensional space using the predictors, defines the coordinates of the outcome data in it and
determines the optimum separation hyperplane between the different outcome classes. To
optimise the separation between the dependent variable categories, while minimising data
overfitting, both the kernel type (*i.e.*, the spatial transformation function) and the cost (*i.e.*,
the penalty for misclassified samples) can be altered. To obtain a SVM predictive model,
we used the R package `kernlab`, specified the kernel function as radial and tuned the value
of the cost parameter.
```{r SVM}
for (i in 41:60) {
  model_performance <- run_support_vector_machines(TT_data, predictors_all, model_performance, "SVM", i)
}
```

**Neural network (NN)** <br>
NN is a machine learning algorithm comprised of simple computing units, *i.e.*, neurons,
organised in interconnected layers, *i.e.*, network, where the predictor layer input passes
through one or more hidden layers before reaching the output layer and determining the
dependent variable value. The algorithm’s architecture, the number of hidden neurons and
layers and the weight decay, *i.e.*, the penalisation for large regression coefficients, among
others, can all be adjusted to optimise the NN predictive performance. In this study, we
used the R package `nnet` to train a NN model with a feed-forward architecture and a
single hidden layer. The optimum number of hidden neurons and the weight decay value
were determined through parameter tuning.

```{r NN}
for (i in 61:80) {
  model_performance <- run_neural_network(TT_data, predictors_all, model_performance, "NN", i)
}
```

#### Model performance comparison
The prediction perfromance of the four models on the testing data (randomly selected 30% of `TT_data`, which was not used for training) was visually assessed using the various perfromance metrics.

```{r melt, echo = FALSE}
model_performance_melt <- melt(model_performance[-c(2)], value.name = "Model")
```

```{r models plot 1, fig.height=4, fig.width=10, echo=FALSE}
ggplot(model_performance_melt, aes(x = Model, y = value,  color = Model)) + 
  geom_boxplot() +
  facet_grid(cols = vars(variable)) +
  scale_x_discrete(limits=c("LR", "RF", "SVM", "NN"))  +
  ylim(0.6,1) +
  labs(title = "Predictive performance of different models on testing data",
       y = "Performance") +
  scale_color_brewer(palette = "Set1") +
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")
```

The results suggest that the logistic regression (LR) model generalizes unseen phase behaviour data significantly worse than the other three models, *i.e.*, random forest (RF), support vector machines (SVM) and neural networks (NN), which, in turn, perform comparatively well.  

### Model selection using AUC metric
To select the best model for the phase behaviour classification problem, the area under the receiver operating characteristic (ROC) curve (AUC) for the predictions on the testing data for the three better-performing models identified above, *i.e.*, RF, SVM and NN, was compared.

**Visual comparison** <br>
```{r AUC plot, fig.height=4, fig.width=10, echo=FALSE}
model_performance$Model <- factor(model_performance$Model)
model_performance$AUC <- as.numeric(model_performance$AUC)

ggplot(model_performance, aes(x = Model, y=AUC)) + 
  geom_boxplot() + 
  scale_x_discrete(limits=c("RF", "SVM", "NN"))  +
  ylim(0.94,1) +
  labs(title = "Area under the ROC curve (AUC)", y = "Performance") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5)) 
```

**Statistical comparison ** <br>
Comparison of AUC testing values for the different models:

```{r models testing statistics, echo = FALSE}
run_statistical_analysys(model_performance, model_performance[21:80,]$AUC, model_performance[21:80,]$Model)
```
<br>

**The results suggest that the random forest and neural network models have a statistically significantly better aggregate predictive performance on the testing datasets than the support vector machines models. While the NN models had a slightly higher average AUC (0.972 ± 0.04) than the RF models (0.971 ± 0.04), the difference between the two sets of models was not statistically significant (p = 0.45). Hence, the less computationally demanting model, *i.e.* random forest, was carried forward.**


```{r export, echo = FALSE}
write.csv(model_performance, "Results/Model performance.csv", row.names = FALSE)
```